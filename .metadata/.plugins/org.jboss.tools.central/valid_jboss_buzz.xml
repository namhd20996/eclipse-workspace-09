<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Enable user-managed networking with ZTP</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/07/enable-user-managed-networking-ztp" /><author><name>Wuxin Zeng</name></author><id>aa148867-32ad-4620-a1b5-4de2345f0f4d</id><updated>2023-07-07T07:00:00Z</updated><published>2023-07-07T07:00:00Z</published><summary type="html">&lt;p&gt;This article demonstrates how to configure user-managed networking via &lt;code&gt;SiteConfig&lt;/code&gt; using the zero-touch provisioning (ZTP) deployment model. ZTP automates the steps required to configure new network devices and perform upgrades using a network switch feature.&lt;/p&gt; &lt;p&gt;This tutorial uses Red Hat Advanced Cluster Management for Kubernetes with the &lt;a href="https://docs.openshift.com/container-platform/4.10/installing/installing_on_prem_assisted/installing-on-prem-assisted.html"&gt;Red Hat OpenShift&lt;/a&gt; Assisted Installer and Git-stored &lt;code&gt;SiteConfig&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Configure the cluster network&lt;/h2&gt; &lt;p&gt;Before installing &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; on-premise with Assisted Installer, we must &lt;a href="https://docs.openshift.com/container-platform/4.11/installing/installing_on_prem_assisted/assisted-installer-installing.html#configuring-networking_assisted-installer-installing"&gt;configure the cluster network&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;There are two options for configuring cluster networks: One is using cluster-managed networking, and the other is user-managed networking.&lt;/p&gt; &lt;p&gt;We select user-managed networking when we want to use a third-party vendor for configuring external load balancers. Therefore, it is crucial that we configure the &lt;code&gt;SiteConfig&lt;/code&gt; to use user-managed networking instead of cluster-managed networking for the Assisted Installer.&lt;/p&gt; &lt;h2&gt;Enable userManagedNetworking using SiteConfigs&lt;/h2&gt; &lt;p&gt;However, it might not be so clear how user-managed networking is enabled via the &lt;code&gt;SiteConfig&lt;/code&gt; (ZTP), and this article aims to clear that confusion.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;SiteConfig&lt;/code&gt; custom resource (CR) generates the other cluster configurations' custom resources during the managed cluster installation. One of the custom resources is the &lt;code&gt;AgentClusterInstall&lt;/code&gt; CR, which is the installation CR and the trigger for any deployment, like the &lt;code&gt;install-config.yaml&lt;/code&gt;. User-managed networking is a flag that can be toggled inside this &lt;code&gt;AgentClusterInstall&lt;/code&gt; CR.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: extensions.hive.openshift.io/v1beta1 kind: AgentClusterInstall metadata: name: "{{ .Cluster.ClusterName }}" namespace: "{{ .Cluster.ClusterName }}" annotations: agent-install.openshift.io/install-config-overrides: "{{ .Cluster.NetworkType }}" argocd.argoproj.io/sync-wave: "1" spec: clusterDeploymentRef: name: "{{ .Cluster.ClusterName }}" holdInstallation: "{{ .Cluster.HoldInstallation }}" imageSetRef: name: "{{ .Cluster.ClusterImageSetNameRef }}" apiVIP: "{{ .Cluster.ApiVIP }}" ingressVIP: "{{ .Cluster.IngressVIP }}" apiVIPs: "{{ .Cluster.ApiVIPs }}" ingressVIPs: "{{ .Cluster.IngressVIPs }}" networking: userManagedNetworking: true clusterNetwork: "{{ .Cluster.ClusterNetwork }}" machineNetwork: "{{ .Cluster.MachineNetwork }}" serviceNetwork: "{{ .Cluster.ServiceNetwork }}" provisionRequirements: controlPlaneAgents: "{{ .Cluster.NumMasters }}" workerAgents: "{{ .Cluster.NumWorkers }}" proxy: "{{ .Cluster.ProxySettings }}" sshPublicKey: "{{ .Site.SshPublicKey }}" manifestsConfigMapRef: name: "{{ .Cluster.ClusterName }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the &lt;code&gt;AgentClusterInstall&lt;/code&gt; CR, we can set the &lt;code&gt;userManagedNetworking&lt;/code&gt; flag to true to configure user-managed networking. Once the flag is set, we can put this override template in the &lt;code&gt;SiteConfig&lt;/code&gt; using the key &lt;code&gt;AgentClusterInstall&lt;/code&gt; under &lt;code&gt;crTemplates&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: ran.openshift.io/v1 kind: SiteConfig ... ... crTemplates: AgentClusterInstall: "AgentClusterInstallOverride.yaml" ... ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This way, we can redefine &lt;code&gt;AgentClusterInstall&lt;/code&gt; to add the key/value &lt;code&gt;userManagedNetworking: true&lt;/code&gt; for networking.&lt;/p&gt; &lt;p&gt;Alternatively, we can directly set the &lt;code&gt;userManagedNetworking&lt;/code&gt; flag to true by using the key &lt;code&gt;installConfigOverrides&lt;/code&gt; inside the &lt;code&gt;SiteConfig&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: ran.openshift.io/v1 kind: SiteConfig metadata: name: "test-site" namespace: "test-site" spec: ... ... clusters: - clusterName: "cluster1" clusterType: sno numMasters: 1 networkType: "OVNKubernetes" installConfigOverrides: "{\"networking\":{\"UserManagedNetworking\":\"true\"}} ... ...&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article explains how to enable user-managed networking via the &lt;code&gt;SiteConfig&lt;/code&gt; custom resource using the &lt;code&gt;AgentClusterInstall&lt;/code&gt; key under &lt;code&gt;crTemplates&lt;/code&gt;, which overrides all the generated CRs. Alternatively, you can set &lt;code&gt;userManagedNetworking: true&lt;/code&gt; in the &lt;code&gt;SiteConfig&lt;/code&gt; using &lt;code&gt;installConfigOverrides&lt;/code&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/07/enable-user-managed-networking-ztp" title="Enable user-managed networking with ZTP"&gt;Enable user-managed networking with ZTP&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Wuxin Zeng</dc:creator><dc:date>2023-07-07T07:00:00Z</dc:date></entry><entry><title>How to use Debezium SMT with Groovy to filter routing events</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/06/how-use-debezium-smt-groovy-filter-routing-events" /><author><name>Diego Neves</name></author><id>4b98c474-c94a-42a4-a72b-dd804d05c761</id><updated>2023-07-06T07:00:00Z</updated><published>2023-07-06T07:00:00Z</published><summary type="html">&lt;p&gt;After configuring my Kafka Connect Image with Debezium, demonstrated in Hugo Guerrero's article &lt;a href="https://developers.redhat.com/articles/2021/12/06/improve-your-kafka-connect-builds-debezium#"&gt;Improve your Kafka Connect builds of Debezium&lt;/a&gt;, I needed to configure a type of filter to only bring certain events from the database table to my topics. I was able to do this using Debezium SMT with Groovy.&lt;/p&gt; &lt;h2&gt;What is Debezium SMT?&lt;/h2&gt; &lt;p&gt;Debezium SMT (single message transform) is a filter feature provided by Debezium that is used to process only records that you find relevant. To do that, you need to include plugins the implementations of the JSR223 API (Scripting for the Java Platform) inside your Kafka Connect Image.&lt;/p&gt; &lt;p&gt;Note that Debezium does not come with an JSR 223 implementation, so you will need to provide the libs to use this feature. We will use the Groovy implementation of JSR 223, so you can download all the relevant jars from the &lt;a href="https://groovy-lang.org/"&gt;Groovy website&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;There are other JSR 223 implementations that you can use, however, we will not cover them here. If you want information about this, go to &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_integration/2021.q3/html-single/debezium_user_guide/index#filtering-debezium-change-event-records"&gt;Debezium documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Download the files&lt;/h2&gt; &lt;p&gt;First of all, you will need your database plugin (i.e., SQL Server or MySQL) from the &lt;a href="https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?downloadType=distributions&amp;product=red.hat.integration&amp;version=2022-Q4"&gt;download page&lt;/a&gt;. Figure 1 illustrates the Red Hat software downloads page.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-03-31_at_10.02.31.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-03-31_at_10.02.31.png?itok=I0rE4fLs" width="600" height="362" alt="A screenshot of the Red Hat software download page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The Red Hat software download page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;That is the connector will need to put in your&lt;em&gt; &lt;/em&gt;Kafka Connect to work with MySQL CDC&lt;em&gt;. &lt;/em&gt;You will also need to download the scripting transformation package.&lt;/p&gt; &lt;p&gt;With this in place, go to the &lt;a href="https://groovy-lang.org/"&gt;Groovy website&lt;/a&gt; and download the zip that contains all the JAR's files, as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-03_at_11.59.38.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-03_at_11.59.38.png?itok=TulJKNlR" width="600" height="160" alt="A screenshot of the Groovy download page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The Groovy download page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 3 shows the three zip files that we will unzip in the next steps.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-03_at_12.04.20.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-03_at_12.04.20.png?itok=BuOaJxBH" width="600" height="183" alt="A screenshot of the zip files dowloaded for Debezium and Groovy." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The zip files dowloaded for Debezium and Groovy.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Creating the image&lt;/h2&gt; &lt;p&gt;Unzip the files dowloaded in the last step. Use the SQL server plugin, as shown in Figure 4.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-03_at_12.09.15.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-03_at_12.09.15.png?itok=xrl0CNz0" width="600" height="203" alt="A screenshot of the unzipped debezium and groovy folders." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The unzipped debezium and groovy folders.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Go to the &lt;strong&gt;debezium-scripting&lt;/strong&gt; folder and copy the debezium-scripting-1.9.7.Final...jar and place it inside the &lt;strong&gt;debezium-connector-sqlserver&lt;/strong&gt; folder.&lt;/p&gt; &lt;p&gt;Then go to the &lt;strong&gt;groovy-4.0.11/lib &lt;/strong&gt;folder and copy the jars groovy-4.0.11.jar and groovy-jsr223-4.0.11.jar. Place them in the &lt;strong&gt;debezium-connector-sqlserver&lt;/strong&gt; folder. At this point, your folder should look like Figure 5. Keep in mind that your versions may be different. These are the versions available at the time of this article.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-03_at_12.07.52.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-03_at_12.07.52.png?itok=q0QC8EEG" width="600" height="348" alt="A screenshot of the plugin folder with all the necessary jars." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: The plugin folder with all the necessary jars.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now, zip the debezium-connector-sqlserver folder and place this zip file into your nexus or Git. Then use this as your artifact, as shown in the previously mentioned Hugo Guerrero &lt;a href="https://developers.redhat.com/articles/2021/12/06/improve-your-kafka-connect-builds-debezium#"&gt;article&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;How to use transformations&lt;/h2&gt; &lt;p&gt;To use this feature, create your Kafka connectors and configure them to use the transformations like the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kind: KafkaConnector apiVersion: kafka.strimzi.io/v1beta2 metadata: name: sql-connector-for-inserts labels: strimzi.io/cluster: my-connect-cluster namespace: kafka spec: class: io.debezium.connector.sqlserver.SqlServerConnector tasksMax: 1 config: database.hostname: "server.earth.svc" database.port: "1433" database.user: "sa" database.password: "Password!" database.dbname: "InternationalDB" table.whitelist: "dbo.Orders" database.history.kafka.bootstrap.servers: "my-cluster-kafka-bootstrap:9092" database.server.name: "internation-db-insert-topic" &lt;-- # This property need to have a unique value database.history.kafka.topic: "dbhistory.internation-db-insert-topic" &lt;-- # This property need to have a unique value #### Here start the transforms feature, using the condition where operation is equal 'c', only #### events of that type will be routed to the topic created by this connector. transforms: filter transforms.filter.language: jsr223.groovy transforms.filter.type: io.debezium.transforms.Filter transforms.filter.condition: value.op == 'c' transforms.filter.topic.regex: internation-db-insert-topic.dbo.Orders\ #### end of transforms filter tombstones.on.delete: 'false'&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;This article demonstrated how to configure a Kafka connect image to use Debezium SMT with Groovy and showed you how to use transformations and filters to route events between topics. For more information, refer to the &lt;a href="https://debezium.io/documentation/reference/1.9/transformations/index.html"&gt;Debezium documentation&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/06/how-use-debezium-smt-groovy-filter-routing-events" title="How to use Debezium SMT with Groovy to filter routing events"&gt;How to use Debezium SMT with Groovy to filter routing events&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Diego Neves</dc:creator><dc:date>2023-07-06T07:00:00Z</dc:date></entry><entry><title>Long-Term Support (LTS) for Quarkus</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/lts-releases/&#xA;            " /><author><name>Max Rydahl Andersen (https://twitter.com/maxandersen)</name></author><id>https://quarkus.io/blog/lts-releases/</id><updated>2023-07-06T00:00:00Z</updated><published>2023-07-06T00:00:00Z</published><summary type="html">We’re thrilled to announce the introduction of Long-Term Support (LTS) releases for Quarkus. We aim to strike a balance between our regular high-paced release cycle and the needs of users who require more stability and predictability. LTS releases offer an opportunity for our community to enjoy a version of Quarkus,...</summary><dc:creator>Max Rydahl Andersen (https://twitter.com/maxandersen)</dc:creator><dc:date>2023-07-06T00:00:00Z</dc:date></entry><entry><title>How to use Kafka Cruise Control for cluster optimization</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/05/how-use-kafka-cruise-control-cluster-optimization" /><author><name>Donato Marrazzo</name></author><id>63131296-8036-4e40-a0a8-2ce85ee747af</id><updated>2023-07-05T07:00:00Z</updated><published>2023-07-05T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.redhat.com/en/resources/amq-streams-datasheet"&gt;AMQ streams&lt;/a&gt; has recently promoted Cruise Control to the General Availability stage. It optimizes how Apache Kafka distributes the workload to improve performance and health. Often the Kafka clusters are deployed and grow over time, hosting multiple topics. Thanks to its robustness and elasticity reputation, the operations department tends to give it little care, monitoring the key health indicators. But they don’t know how to tune it to face the new workload.&lt;/p&gt; &lt;p&gt;Cruise Control can become a fundamental ally in managing your Kafka clusters and getting the most out of your hardware resources. Plus, with the AMQ streams operator, it’s just a matter of turning a key. This article explains the key principles and how to make practical use of this new exciting capability.&lt;/p&gt; &lt;h2&gt;Unbalanced workloads&lt;/h2&gt; &lt;p&gt;When a new topic is created in Kafka, the partitions and its replicas are distributed evenly among the available brokers in the cluster. This is a wise behavior if your cluster is empty and you have no idea of your actual workload for different partitions. Figure 1 shows an example of how eight topics with three partitions are distributed across a cluster of three brokers. Partitions belonging to the same topic have the same color.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/equal-partitions_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/equal-partitions_0.png?itok=ZEXJShGN" width="600" height="123" alt="An illustration of a topic with eight partitions and three replicas that is distributed across a cluster of three brokers with equal partitions." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: How 8 topics with 3 equal partitions are distributed across a cluster of 3 brokers.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Over time, layering multiple topics from different applications can result in partitions with different sizes and workloads. Simply increasing the size of the cluster does not solve the problem. In fact, the new cluster member will be used along with the existing ones to accommodate newly created topic partitions, but it won’t change the assignment of the existing topics.&lt;/p&gt; &lt;p&gt;In Figure 2, a cluster running out of resources expands with a new broker, then the user adds another topic with two partitions. New partitions are assigned in a round-robin fashion. So some will be assigned to the new broker, but the overloaded brokers are not relieved.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/scaled-unbalanced.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/scaled-unbalanced.png?itok=ueDaIiCX" width="600" height="149" alt="An illustration of a new broker hosting only new partition replicas." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The new broker hosts only new partition replicas.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;A Kafka cluster can be unbalanced from these different points of view:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Network utilization&lt;/li&gt; &lt;li&gt;RAM and CPU utilization&lt;/li&gt; &lt;li&gt;Disk utilization&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;There are tools that allow the administrator to selectively reassign partitions across the cluster (&lt;code&gt;kafka-reassign-partitions.sh&lt;/code&gt;), but this approach might work if there is a clear idea of the root cause of the unbalanced workload. Moreover, there are also other requirements that you need to address:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Replicas of the same partition must be in different racks.&lt;/li&gt; &lt;li&gt;All the physical resources cannot be exhausted (maximum capacity for disk, network, CPU).&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Trying to improve the partition assignment manually is tedious and error prone. Moreover, when the number of options grows, the mathematical optimization theory tells us that it will also lead to poorly optimized solutions. In fact, those problems are well known as mathematical optimization problems. The theory explains that even with a limited number of variables, the search space can become so huge that even the most advanced computer would take centuries to find the optimal solution. In fact, they fall under the &lt;a href="https://en.wikipedia.org/wiki/NP-completeness"&gt;NP-Complete&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/NP-hardness"&gt;NP-hard&lt;/a&gt; problems.&lt;/p&gt; &lt;p&gt;Fortunately, mathematicians and AI experts found methods (algorithms) to find at least a good enough solution to those problems (near optimal).&lt;/p&gt; &lt;h2&gt;Cruise Control for Apache Kafka&lt;/h2&gt; &lt;p&gt;LinkedIn, who originally created Apache Kafka and operates it on a large scale, developed &lt;a href="https://github.com/linkedin/cruise-control"&gt;Cruise Control&lt;/a&gt; to keep their clusters healthy. Then they made it open source.&lt;/p&gt; &lt;p&gt;Here is a summary of the key features of Kafka Cruise Control:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Resource utilization tracking&lt;/strong&gt; for brokers, topics, and partitions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-goal rebalance proposal generation&lt;/strong&gt; (subset) &lt;ul&gt;&lt;li&gt;Rack-awareness&lt;/li&gt; &lt;li&gt;Resource capacity violation checks (CPU, DISK, Network I/O)&lt;/li&gt; &lt;li&gt;Per-broker replica count violation check&lt;/li&gt; &lt;li&gt;Resource utilization balance (CPU, DISK, Network I/O)&lt;/li&gt; &lt;li&gt;Leader traffic distribution&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Actualize the previous proposal&lt;/strong&gt;: &lt;ul&gt;&lt;li&gt;Rebalance the current partition topology&lt;/li&gt; &lt;li&gt;Rebelance on newly added brokers&lt;/li&gt; &lt;li&gt;Rebalance before removing brokers&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;AMQ streams makes Cruise Control truly accessible, especially within the &lt;a href="https://www.redhat.com/en/resources/openshift-container-platform-datasheet"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;. In fact, the operator provides an easy way to deploy Cruise Control and introduces a declarative way to trigger the analysis and apply rebalance proposals.&lt;/p&gt; &lt;h2&gt;Rebalance the cluster&lt;/h2&gt; &lt;p&gt;Cruise Control is a sophisticated tool, with many options that allow the administrator to tailor it to his specific environment.&lt;/p&gt; &lt;p&gt;Before applying it to your production environment, it’s recommended to understand in detail all the features that are widely discussed in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq_streams/2.3/html-single/configuring_amq_streams_on_openshift.html/index#cruise-control-concepts-str"&gt;official documentation&lt;/a&gt;. For this article, we’ll use the default configuration, which already provides an excellent experience with this new feature and a good understanding of the overall process.&lt;/p&gt; &lt;h2&gt;Install Cruise Control&lt;/h2&gt; &lt;p&gt;In the OpenShift Container Platform, enabling the Cruise Control is a matter of adding a line to your normal configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="sh language-bash"&gt;oc patch kafka my-cluster --patch '{"spec":{"cruiseControl": {}}}' --type=merge &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Behind the scenes, a new pod running Cruise Control is launched, and the Kafka cluster is instrumented to collect the required metrics, which are finally delivered through a set of new dedicated topics. It’s worth mentioning that to inject the metrics reporter, the Kafka pods go through a rolling update and in such a way that it preserves service continuity.&lt;/p&gt; &lt;h3&gt;Simulate an unbalanced workload&lt;/h3&gt; &lt;p&gt;One of the challenges with this feature is testing it in a reproducible manner, so you may be wondering how to achieve an unbalanced cluster in your test environment. A rather simple way, I found, is to develop a Kafka producer that intentionally generates loads against a subset of partitions whose leader is hosted on a particular broker. In fact, the broker that is the leader for a given partition is much more stressed than those that act as partition followers. Monitoring the CPU and network, you should get something that resembles Figure 3.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cpu_network_unbalanced.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/cpu_network_unbalanced.png?itok=4yi5UQcr" width="600" height="193" alt="A screenshot of a CPU and network chart in Grafana." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The CPU and Network chart in Grafana.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In Figure 3, the broker named &lt;code&gt;my-cluster-kafka-0&lt;/code&gt; differs from the others in both graphs. It scores less in the &lt;strong&gt;Network Idle&lt;/strong&gt; graph, while it scores more in the &lt;strong&gt;CPU Usage&lt;/strong&gt; graph.&lt;/p&gt; &lt;h3&gt;Start rebalancing&lt;/h3&gt; &lt;p&gt;The following is a simple procedure to engage the Cruise Control and rebalance the cluster:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Deploy a basic rebalance configuration: &lt;pre&gt; &lt;code class="sh language-bash"&gt;echo " apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaRebalance metadata: name: full-rebalance labels: strimzi.io/cluster: my-cluster spec: {} " | oc apply -f - &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;When &lt;code&gt;KafkaRebalance&lt;/code&gt; is deployed, the Cruise Control immediately analyzes the Kafka metrics and prepares an optimization proposal. In fact, the following command shows &lt;code&gt;True&lt;/code&gt; under the &lt;code&gt;PROPOSALREADY&lt;/code&gt; column: &lt;pre&gt; &lt;code class="sh language-bash"&gt;oc get kafkarebalance NAME CLUSTER PENDINGPROPOSAL PROPOSALREADY REBALANCING READY NOTREADY full-rebalance my-cluster True &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;To finally kick off the optimization, you need to approve the proposal by annotating the &lt;code&gt;KafkaRebalance&lt;/code&gt; resource: &lt;pre&gt; &lt;code class="sh language-bash"&gt;oc annotate kafkarebalance full-rebalance strimzi.io/rebalance=approve &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you want to trigger the rebalance directly without any further approval step, you can add the following annotation:&lt;/p&gt; &lt;pre&gt; &lt;code class="sh language-bash"&gt;oc annotate kafkarebalance full-rebalance strimzi.io/rebalance-auto-approval=true &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;After a few minutes, to collect enough data and stabilize the workload, you should be able to evaluate the results as shown in Figure 4, where the lines of the different cluster members tend to equalize. &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cpu_network_rebalanced.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/cpu_network_rebalanced.png?itok=-tJhG6eK" width="600" height="193" alt="A screenshot of the CPU and network chart in Grafana after rebalance." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The CPU and network chart in Grafana after rebalance.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Scale up and rebalance&lt;/h3&gt; &lt;p&gt;Let’s consider the situation where you need to scale out the current cluster and take immediate advantage of the newly added broker.&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Scale out the Kafka cluster: &lt;pre&gt; &lt;code class="sh language-bash"&gt;oc patch kafka my-cluster --patch '{"spec":{"kafka": {"replicas": 4}}}' --type=merge &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;The following configuration will ask the Cruise Control to redistribute the workload by explicitly taking the new cluster member into account: &lt;pre&gt; &lt;code class="sh language-bash"&gt;echo " apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaRebalance metadata: name: add-brokers-rebalance labels: strimzi.io/cluster: my-cluster spec: mode: add-brokers brokers: [3] " | oc apply -f - &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The previous &lt;code&gt;KafkaRebalance&lt;/code&gt; introduced two new &lt;code&gt;spec&lt;/code&gt; properties: &lt;code&gt;mode: add-brokers&lt;/code&gt; and &lt;code&gt;brokers: [3]&lt;/code&gt;. Their purpose is to make Cruise Control aware of the existence of the newly added broker and instruct it to distribute the workload to the newcomer.&lt;/p&gt; &lt;/li&gt; &lt;li&gt;Before approving the rebalance proposal, let’s understand some other details issuing &lt;code&gt;oc describe kafkarebalance add-brokers-rebalance&lt;/code&gt;: &lt;pre&gt; &lt;code class="sh"&gt;(...) Data To Move MB: 17 Excluded Brokers For Leadership: Excluded Brokers For Replica Move: Excluded Topics: Intra Broker Data To Move MB: 0 Monitored Partitions Percentage: 100 Num Intra Broker Replica Movements: 0 Num Leader Movements: 0 Num Replica Movements: 89 On Demand Balancedness Score After: 80.87946050436929 On Demand Balancedness Score Before: 76.41773549482696 (...) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It indicates that is going to move 17MB of data and 89 partition replicas. This will lead to a scoring improvement of around four points.&lt;/p&gt; &lt;p&gt;You might wonder about the &lt;strong&gt;score value&lt;/strong&gt;. The optimization algorithms translate the different goals into mathematical functions that measure how good a solution is with respect to the given goals. So the number alone doesn’t mean much, but you should expect that the scoring after optimization is increased. For the sake of accuracy, not all the Cruise Control actions contribute to the score. Therefore, the rebalancing activity may improve the health of the cluster, even if the score does not change.&lt;/p&gt; &lt;/li&gt; &lt;li&gt;Approve the optimization: &lt;pre&gt; &lt;code class="sh language-bash"&gt;oc annotate kafkarebalance add-brokers-rebalance strimzi.io/rebalance=approve &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Later on, if you want to repeat the analysis and then trigger a new optimization, all you have to do is add another annotation:&lt;/p&gt; &lt;pre&gt; &lt;code class="sh language-bash"&gt;oc annotate kafkarebalance add-brokers-rebalance strimzi.io/rebalance=refresh &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In a production environment, the rebalancing process might take some time, but the cluster remains operational. Your clients could experience a brief pause (a few seconds) as their partitions move elsewhere and they have to reestablish communication with a new broker. However, you may prefer to rebalance when the load is lighter. In these cases, if the rebalancing is taking too long and the peak time is approaching, you can stop the ongoing optimization. The rebalancing effort is broken down into a series of concatenated batches. So when a stop is needed, the following annotation informs the Cruise Control that the next batch must not start, whereas the running one is completed:&lt;/p&gt; &lt;pre&gt; &lt;code class="sh language-bash"&gt;oc annotate kafkarebalance add-brokers-rebalance strimzi.io/rebalance=stop &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you want to test this feature in your demo environment, you can follow the detailed instructions of my AMQ streams demo project on &lt;a href="https://github.com/dmarrazzo/amq-streams-demo"&gt;GitHub&lt;/a&gt;. It will guide you through a Kafka cluster deployment using the Grafana dashboard to inspect the workload. Then you will run an application capable of generating an unbalanced load, and finally you will ponder the advantages of the rebalancing.&lt;/p&gt; &lt;h2&gt;Explore Kafka Cruise Control benefits&lt;/h2&gt; &lt;p&gt;Cruise Control is a valuable companion to Kafka, especially as your environment matures and evolves. AMQ streams makes it easy to take advantage of its many benefits, so it’s definitely worth adding it to your cluster. Clearly, before deploying it into a production environment, I recommend reading the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq_streams/2.3/html-single/configuring_amq_streams_on_openshift.html/index#cruise-control-concepts-str"&gt;official documentation&lt;/a&gt;, understand the available options (e.g., filtering out rebalancing goals), and monitor the reassignment closely.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/05/how-use-kafka-cruise-control-cluster-optimization" title="How to use Kafka Cruise Control for cluster optimization"&gt;How to use Kafka Cruise Control for cluster optimization&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Donato Marrazzo</dc:creator><dc:date>2023-07-05T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.16.8.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-16-8-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-16-8-final-released/</id><updated>2023-07-05T00:00:00Z</updated><published>2023-07-05T00:00:00Z</published><summary type="html">As mentioned in previous blog posts, we encourage all our users to upgrade to Quarkus 3. However we understand the migration can require some time so we will continue to maintain 2.16.x for a while. Today, we released Quarkus 2.16.8.Final, the eighth maintenance release of our 2.16 release train. As...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-07-05T00:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.40.1 released!</title><link rel="alternate" href="https://blog.kie.org/2023/07/kogito-1-40-1-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/07/kogito-1-40-1-released.html</id><updated>2023-07-04T13:41:52Z</updated><content type="html">We are glad to announce that the Kogito 1.40.1 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Start CloudEvent attributes and extensions are accessible as KogitoProcessContext.headers().  * Character “-” is supported in serverless workflow identifier string.  * If there is a parsing error in one of the BPMN/SWF resources, the build procedure can continue if kogito.codegen.process.failOnError property is set to false. * Logging improvements. * When a message publishing failure is detected, a log is printed. * If sysout or script custom type are missing a mandatory attribute, a more user friendly exception is thrown.  * Added SSL support for REST custom type.  * Embedded Serverless Workflow executor. Kafka support * Added CallbackState. * Added EventState.  * Added event publishing.  * Fix issue when constant string specified as message of Sysout action is a single word.   * Swagger documentation is properly generated when the output schema is present. * SWF Project (single file) to CR generator * Implement the Knative Addressable interface in dev profile * Enforce ConfigMap mount path for files referenced by the function definition * Cannot create Workflow on Openshift if spec.platform.registry is set * Do not fail the Dev mode Container Image if the workflow is invalid * KSW Discovery – You need to define a KubernetesServiceCatalog implementation * Add dataindex to usecase example working with knative eventing * Allow Data-index to consume different kind of eventing * Avoided Data Index warnings about OpenAPI duplicated operationIds For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.30.0 artifacts are available at the . A detailed changelog for 1.40.0 can be found in as well as . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>A developer’s guide to secure coding with FORTIFY_SOURCE</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/04/developers-guide-secure-coding-fortifysource" /><author><name>Sandipan Roy</name></author><id>918ec339-c4e6-46d8-90a1-4892a2b0ff47</id><updated>2023-07-04T07:00:00Z</updated><published>2023-07-04T07:00:00Z</published><summary type="html">&lt;p&gt;Secure coding is essential to building robust and resilient software that is less susceptible to exploitation by attackers. One way to ensure secure coding is to use a feature called FORTIFY_SOURCE. In this article, we will explore FORTIFY_SOURCE and how it can be used to enhance the security of your code.&lt;/p&gt; &lt;h2&gt;What is FORTIFY_SOURCE?&lt;/h2&gt; &lt;p&gt;FORTIFY_SOURCE is a feature available in the GNU C Library that provides runtime protection against certain types of security vulnerabilities. Specifically, FORTIFY_SOURCE detects and prevents buffer overflow and formats string vulnerabilities, which are two common types of vulnerabilities that attackers can exploit to take control of a system or steal sensitive data.&lt;/p&gt; &lt;h2&gt;How does FORTIFY_SOURCE work?&lt;/h2&gt; &lt;p&gt;FORTIFY_SOURCE works by providing enhanced versions of certain C library functions that can detect when a buffer overflow or format string vulnerability is about to occur. When a vulnerable function is called, FORTIFY_SOURCE checks the size of the buffer being used and ensures that it is not being overrun. If an overflow or vulnerability is detected, FORTIFY_SOURCE immediately terminates the program to prevent further damage.&lt;/p&gt; &lt;p&gt;For example, consider the following code snippet:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;char buffer[8]; strcpy(buffer, "hello world");&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this code, the &lt;code&gt;strcpy&lt;/code&gt; function is used to copy the "hello world" string into the &lt;code&gt;buffer&lt;/code&gt; variable. However, the &lt;code&gt;buffer&lt;/code&gt; variable is only allocated eight bytes of memory, which is not enough to hold the entire string. This results in a buffer overflow vulnerability that can be exploited by attackers.&lt;/p&gt; &lt;p&gt;If FORTIFY_SOURCE is enabled, the &lt;code&gt;strcpy&lt;/code&gt; function is replaced by a secure version that checks the size of the buffer and prevents an overflow from occurring. In this case, the program would terminate before the vulnerability could be exploited.&lt;/p&gt; &lt;p&gt;Consider another code snippet:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;char password[16]; scanf("%s", password);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this code, the &lt;code&gt;scanf&lt;/code&gt; function is used to read input from the user and store it in the &lt;code&gt;password&lt;/code&gt; variable. However, the &lt;code&gt;scanf&lt;/code&gt; function does not perform any bounds checking, which means that if the user enters more than 16 characters, a buffer overflow vulnerability could occur.&lt;/p&gt; &lt;p&gt;To mitigate this vulnerability using FORTIFY_SOURCE, you can use the secure version of &lt;code&gt;scanf&lt;/code&gt;, called &lt;code&gt;scanf_s&lt;/code&gt;, which checks the size of the buffer and prevents an overflow from occurring. Here's how the code would look using &lt;code&gt;scanf_s&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;char password[16]; scanf_s("%15s", password, sizeof(password));&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this code, &lt;code&gt;scanf_s&lt;/code&gt; takes an additional parameter that specifies the maximum number of characters that can be read from the user. In this case, we set the maximum length to 15, which leaves one byte for the null terminator that is added to the end of the string.&lt;/p&gt; &lt;p&gt;By using &lt;code&gt;scanf_s&lt;/code&gt; instead of &lt;code&gt;scanf&lt;/code&gt;, we can prevent buffer overflow vulnerabilities in our code.&lt;/p&gt; &lt;h2&gt;How to use FORTIFY_SOURCE&lt;/h2&gt; &lt;p&gt;To use FORTIFY_SOURCE in your code, you must first ensure that it is enabled in your development environment. FORTIFY_SOURCE is &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;typically enabled or disabled by invoking &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;D_FORTIFY_SOURCE &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;compiler flags. I&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;t is enabled by default in rpm macros and used when building all packages, but other uses of GCC need to explicitly enable it.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Follow these steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Compile your code using a compiler that supports FORTIFY_SOURCE. Most modern C compilers, such as GCC and Clang, support this feature.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Enable FORTIFY_SOURCE using the appropriate compiler flag. The flag may vary depending on your compiler and version, but for GCC, you can use the &lt;code&gt;-D_FORTIFY_SOURCE=2&lt;/code&gt; flag to enable the feature.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Compile your code with the appropriate optimization level. FORTIFY_SOURCE is most effective when the code is compiled with optimization enabled, so be sure to use at least &lt;code&gt;-O1&lt;/code&gt; optimization level.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Here's an example command to compile your code with FORTIFY_SOURCE enabled using GCC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;gcc -D_FORTIFY_SOURCE=2 -O1 -o myprogram myprogram.c&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once your code is compiled with FORTIFY_SOURCE enabled, the enhanced secure library functions provided by the feature will automatically replace the standard C library functions, such as &lt;code&gt;strcpy&lt;/code&gt;, &lt;code&gt;scanf&lt;/code&gt;, and &lt;code&gt;printf&lt;/code&gt;. This means that any calls to these functions in your code will be automatically replaced with the secure versions provided by FORTIFY_SOURCE.&lt;/p&gt; &lt;p&gt;Once FORTIFY_SOURCE is enabled, you can use the enhanced secure library functions provided by the feature instead of the standard C library functions. For example, you can use &lt;code&gt;strncpy_s&lt;/code&gt; instead of &lt;code&gt;strcpy&lt;/code&gt; to safely copy a string into a buffer.&lt;/p&gt; &lt;p&gt;It is important to note that FORTIFY_SOURCE does not provide complete protection against all types of security vulnerabilities. It only protects against buffer overflow and format string vulnerabilities. Therefore, it is important to use other secure coding practices in conjunction with FORTIFY_SOURCE to ensure that your code is as secure as possible.&lt;/p&gt; &lt;h2&gt;Advanced usage of FORTIFY_SOURCE&lt;/h2&gt; &lt;p&gt;When using FORTIFY_SOURCE, you can specify a level of protection between 0 and 3. The higher the level, the more security features are enabled. The default level is 1.&lt;/p&gt; &lt;p&gt;FORTIFY_SOURCE=3 provides the highest level of protection and includes all the security features of levels 1 and 2, plus additional checks for potentially dangerous constructs in the code. These additional checks are designed to detect a wider range of security issues, including:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Dangerous use of memcpy and memmove functions.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Dangerous use of snprintf, vsnprintf, and similar functions.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Dangerous use of string manipulation functions like strtok, strncat, and strpbrk.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;However, it's important to note that enabling FORTIFY_SOURCE=3 may have some performance implications, as it adds additional code to perform the security checks. &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Therefore, it might be desirable to use a lower level of protection in performance critical code but the programmer must be mindful of additional security risks.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;To enable FORTIFY_SOURCE=3, you can use the &lt;code&gt;-O2&lt;/code&gt; optimization level in addition to the &lt;code&gt;-D_FORTIFY_SOURCE=3&lt;/code&gt; flag when compiling your code with GCC. Here's an example command to enable FORTIFY_SOURCE=3:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;gcc -D_FORTIFY_SOURCE=3 -O2 -o myprogram myprogram.c&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;FORTIFY_SOURCE=3 provides the highest level of protection against security issues in C and C++ programs, but it may come at a performance cost. Therefore, it's important to carefully consider the level of protection you need and balance it with the performance requirements of your application.&lt;/p&gt; &lt;h3&gt;How to check FORTIFY_SOURCE&lt;/h3&gt; &lt;p&gt;In this example, we're copying a string that is longer than the size of the &lt;code&gt;buf&lt;/code&gt; array, which can lead to a buffer overflow if FORTIFY_SOURCE is not enabled.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#include &lt;stdio.h&gt; #include &lt;string.h&gt; int main() { char buf[10]; strcpy(buf, "1234567890"); printf("%s\n", buf); return 0; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To enable FORTIFY_SOURCE, we can compile the code with the &lt;code&gt;-O2&lt;/code&gt; optimization flag and the &lt;code&gt;-D_FORTIFY_SOURCE=2&lt;/code&gt; preprocessor flag:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;gcc -O2 -D_FORTIFY_SOURCE=2 -o test test.c&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now when we run the program, we should see a runtime error indicating that a buffer overflow has occurred.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;*** buffer overflow detected ***: ./test terminated Aborted (core dumped)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also, we can use &lt;code&gt;objdump&lt;/code&gt; to examine the compiled binary and look for references to FORTIFY_SOURCE. We can use the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;objdump -R test&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will show us the dynamic relocations table for our binary, which includes information about any shared libraries or symbols used by the program.&lt;/p&gt; &lt;p&gt;If FORTIFY_SOURCE is enabled, we should see a reference to the symbol &lt;code&gt;__strcpy_chk&lt;/code&gt; in the relocations table, which is a fortified version of the &lt;code&gt;strcpy&lt;/code&gt; function that performs runtime checks for buffer overflows.&lt;/p&gt; &lt;p&gt;Here's an example of what the output might look like if FORTIFY_SOURCE is enabled:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;test: file format elf64-x86-64 DYNAMIC RELOCATION RECORDS ... 0000000000601018 R_X86_64_JUMP_SLOT __strcpy_chk@GLIBC_2.3.4 ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This indicates that our program is using the fortified &lt;code&gt;__strcpy_chk&lt;/code&gt; function, which is provided by the GNU C library and performs runtime checks to prevent buffer overflows.&lt;/p&gt; &lt;p&gt;Examining the dynamic relocation table of our compiled binary with &lt;code&gt;objdump&lt;/code&gt;, we can check if FORTIFY_SOURCE is enabled and ensure that our code is properly secured against common security issues.&lt;/p&gt; &lt;p&gt;We can also use &lt;code&gt;checksec&lt;/code&gt; tool which primarily used for assessing the security features and hardening options of an executable or shared object file. While it is not specifically designed to check for the presence of FORTIFY_SOURCE in a binary, it can provide valuable information about the overall security posture of the program.&lt;/p&gt; &lt;p&gt;When using &lt;code&gt;checksec&lt;/code&gt; to assess a binary that has been compiled with FORTIFY_SOURCE, it can indicate the presence of certain security features that are commonly enabled by FORTIFY_SOURCE, such as stack canaries or enhanced buffer overflow protections. &lt;code&gt;checksec&lt;/code&gt; can also detect other security mitigations that may have been enabled during compilation, such as Address Space Layout Randomization (ASLR) or Data Execution Prevention (DEP).&lt;/p&gt; &lt;p&gt;Lets see the output of running checksec against our test program:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;$ checksec --file=./test RELRO STACK CANARY NX PIE RPATH RUNPATH FILE Full RELRO Canary found NX enabled PIE enabled No RPATH No RUNPATH test &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the output, we can observe the following several security features being reported.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Full RELRO&lt;/strong&gt; indicates that all relocations have been resolved at load time, providing protection against certain types of attacks like GOT (Global Offset Table) overwrite attacks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Canary found&lt;/strong&gt;: The presence of a stack canary indicates the usage of a security mechanism designed to detect stack-based buffer overflows. FORTIFY_SOURCE often enables stack canaries to protect against these types of vulnerabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NX enabled&lt;/strong&gt;: NX (Non-Executable) marking prevents the execution of code in memory regions that are intended for data. This feature enhances security by preventing the execution of injected or malicious code.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PIE enabled&lt;/strong&gt;: Position Independent Executable (PIE) makes the binary's base address random, providing Address Space Layout Randomization (ASLR) to thwart memory-based attacks. Fortify Source can be used in combination with PIE to strengthen the overall security of the executable.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;While this output doesn't explicitly state the usage of FORTIFY_SOURCE, the presence of stack canaries and other security features suggests that the binary may have been compiled with FORTIFY_SOURCE or similar security-enhancing techniques. To confirm the usage of FORTIFY_SOURCE, it's best to refer to the build configuration or examine the compiler flags and options used during the compilation process.&lt;/p&gt; &lt;h2&gt;FORTIFY_SOURCE improves code security&lt;/h2&gt; &lt;p&gt;FORTIFY_SOURCE is a valuable feature that can enhance the security of your code by providing runtime protection against buffer overflow and format string vulnerabilities. By enabling FORTIFY_SOURCE in your development environment and using secure library functions, you can reduce the risk of security vulnerabilities in your code. Remember that FORTIFY_SOURCE is just one tool in your toolbox and should be used in conjunction with other secure coding practices to ensure the security of your code.&lt;/p&gt; &lt;p&gt;For more information, refer to these articles:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/security-technologies-fortifysource" target="_blank"&gt;Security Technologies: FORTIFY_SOURCE&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/enhance-application-security-fortifysource" target="_blank"&gt;Enhance application security with FORTIFY_SOURCE&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2021/04/16/broadening-compiler-checks-for-buffer-overflows-in-_fortify_source" target="_blank"&gt;Broadening compiler checks for buffer overflows in FORTIFYSOURCE&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/hardening-elf-binaries-using-relocation-read-only-relro"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Hardening ELF binaries using Relocation Read-Only (RELRO)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/04/developers-guide-secure-coding-fortifysource" title="A developer’s guide to secure coding with FORTIFY_SOURCE"&gt;A developer’s guide to secure coding with FORTIFY_SOURCE&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Sandipan Roy</dc:creator><dc:date>2023-07-04T07:00:00Z</dc:date></entry><entry><title type="html">Camel Routes support in Serverless Workflow Editors</title><link rel="alternate" href="https://blog.kie.org/2023/07/camel-routes-support-in-serverless-workflow-editors.html" /><author><name>Saravana Balaji</name></author><id>https://blog.kie.org/2023/07/camel-routes-support-in-serverless-workflow-editors.html</id><updated>2023-07-03T12:49:44Z</updated><content type="html">The Serverless Workflow Editor now offers autocomplete features for functions and events via Catalog Explorer. In my previous blog posts, we saw how and support OpenAPI and AsyncAPI specifications.The same feature now works with Camel Routes as well. While editing Serverless Workflow files, you can register Camel Routes and pass them as functions. These functions will be available on the Serverless Workflow Editor’s Service Catalog Explorer. This article explains how that works. A Camel Route is a set of processing steps that are applied to a message as it travels from a source to a destination.  A Route typically consists of a series of processing steps that are connected in a linear sequence. At this point, Camel Routes are supported by KIE Serverless Workflow VS Code extension and soon the feature will be available on Web Tools as well. INSTALLING THE EXTENSION To proceed further, you will need and the . There are several ways to download and install the KIE Serverless Workflow VS Code extension 1. Download it from the . 2. Launch VS Code Quick Open (Ctrl+P), paste the following commands, and press enter: * ext install kie-group.swf-vscode-extension; 3. Click on the Extensions icon in the Activity Bar on the side of VS Code, search and install. * CAMEL ROUTE SAMPLE After installing the KIE Serverless Workflow VS Code Extension, open your Serverless Workflow project on your workspace and create a folder named “routes” inside the “main/properties” folder. If you have a Camel Routes file, you can place it there. This folder must only contain files that have Camel Routes content. This will be automatically read by the extension and populated on the editor. You can also copy the below sample content and paste it on a file that you can create inside the routes folder. - from: uri: direct:numberToWords steps: - bean: beanType: java.math.BigInteger method: valueOf - setHeader: name: operationName constant: NumberToWords - toD: uri: cxf://example.com?serviceClass=com.dataaccess.webservicesserver.NumberConversionSoapType&amp;amp;wsdlURL=/wsdl/numberconversion.wsdl CAMEL ROUTES AUTOCOMPLETE Once you have the file containing Camel Routes in place, the editor is intelligent enough to automatically read through the files and parse the Camel Routes. Here is a short demo showing how it works. Notice the new custom function with a new operation type. The operation is a URI scheme composed by the constant “camel:”, the “direct:” endpoint, and its name. The Serverless Workflow Editor only supports producing messages to a endpoint at this time. This function can be referenced in other definitions like state or events. To explore and understand the Serverless Workflow Editor with the help of samples, you can try out sample projects from. That’s all for now! There are more exciting features coming up in this space, stay tuned! The post appeared first on .</content><dc:creator>Saravana Balaji</dc:creator></entry><entry><title type="html">Migrating from Java 8 to Java 17 with OpenRewrite</title><link rel="alternate" href="https://www.mastertheboss.com/java/migrating-from-java-8-to-java-17-with-openrewrite/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/java/migrating-from-java-8-to-java-17-with-openrewrite/</id><updated>2023-07-03T12:07:29Z</updated><content type="html">In this article we will discuss on how to migrate Java applications from Java 8 to Java 17 using the OpenRewrite migration plugin. At the end of this tutorial, you will learn which are most common challenges that you can face when upgrading to Java 17 Java 8 to Java 17 challenges In pure project ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Managing Java containers with Quarkus and Podman Desktop</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/03/managing-java-containers-quarkus-and-podman-desktop" /><author><name>Kevin Dubois</name></author><id>8583d285-7edc-424d-b23b-c34c531c6661</id><updated>2023-07-03T07:00:00Z</updated><published>2023-07-03T07:00:00Z</published><summary type="html">&lt;p&gt;The world of software development is changing quickly, and it is no different for the &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; developer who needs to learn new skills to manage Java containers.&lt;/p&gt; &lt;p&gt;In the traditional Java development world, it was rather typical to build a Java artifact (whether it be a .jar, .war, or .ear), and "throw it over the wall" for the middleware and operations people to take care of. It kind of made sense, too, because configuring and running enterprise application servers was not always that easy, and in the end, the developer's laptop would probably not match the specs of staging or production environments so that the configuration would be different anyway. We all know the "It works on my machine" cliché, after all.&lt;/p&gt; &lt;p&gt;These days, things need faster and better delivery, and production issues often require resolution in a very timely manner because every second of downtime could potentially cause revenue loss for an organization. New ways of delivering applications have been conceived—and with it, cloud-native applications, &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt;—and the "but it worked on my machine" excuse is no longer valid.&lt;/p&gt; &lt;p&gt;Java developers need to be able to work with containers because it is a practical way to test applications in a production-like environment on their local machine. Some developers and organizations choose to take development off the local machine and right into the cloud with the IDE itself running in a reproducible, containerized environment for even greater consistency. &lt;a href="https://developers.redhat.com/products/openshift-dev-spaces/overview"&gt;Red Hat OpenShift Dev Spaces&lt;/a&gt; is one way you can do this, but it is out of the scope of this article.&lt;/p&gt; &lt;h2&gt;Building containers with Java&lt;/h2&gt; &lt;p&gt;Building and managing containers can become overwhelming rather quickly for the Java developer, though. At the face of it, it's "just" a matter of creating a Dockerfile with instructions to copy the artifact into a base image and doing a Docker build. However, in reality, creating a container image from an application requires many more considerations than that.&lt;/p&gt; &lt;h3&gt;Choosing a base image&lt;/h3&gt; &lt;p&gt;To start with, you will need a base image: do you start from scratch, adding every dependency individually, or use a base image that already has a Java Virtual Machine (JVM) implementation included? Or maybe you've opted to do a &lt;a href="https://quarkus.io/guides/building-native-image"&gt;native build&lt;/a&gt; of the application and don't need a JVM running in your container. Either way, you will need to find a trusted source that provides tested, secured, ideally signed, &lt;a href="https://opencontainers.org/about/overview/"&gt;Open Container Initiative (OCI)&lt;/a&gt;-compliant, and stable images for the particular Java version you are targeting. And hopefully, this source can provide support and maintain and backport patches going forward. The base image will likely need tuning as well to optimize performance, not something every developer is familiar with.&lt;/p&gt; &lt;p&gt;Red Hat's &lt;a href="https://www.redhat.com/en/blog/introducing-red-hat-universal-base-image"&gt;Universal Base Images (UBI)&lt;/a&gt; are a good starting point for building container images. UBIs are &lt;a href="https://developers.redhat.com/articles/ubi-faq#ubi_details"&gt;free to use&lt;/a&gt;, and they are at the core a subset of the Red Hat Linux (RHEL) distribution, providing a stable, secure, optimized and reliable base for building containers. There are several &lt;a href="https://catalog.redhat.com/software/containers/search?q=openjdk"&gt; JVM-based UBI images&lt;/a&gt; available, as well as micro UBIs for deploying native binaries.&lt;/p&gt; &lt;h3&gt;Building the container image&lt;/h3&gt; &lt;p&gt;Once you have decided on a base image, you will need a way to build the container image. The best-known way is to create a Dockerfile (or Containerfile as the more agnostic way of naming such a file) and then build it with Docker.&lt;/p&gt; &lt;p&gt;This is certainly not the only way to build containers, though. You can use the same Containerfile to build an image with Podman, a fully open source container engine with actually &lt;a href="https://docs.podman.io/en/latest/"&gt;quite a few more features than the Docker engine&lt;/a&gt;. You could also use &lt;a href="https://buildah.io/"&gt;Buildah&lt;/a&gt;, another open source tool specifically created for building container images.&lt;/p&gt; &lt;p&gt;There are alternative ways of building containers without needing a Containerfile. There are build tools such as &lt;a href="https://github.com/openshift/source-to-image"&gt;Source2Image (S2I)&lt;/a&gt;, &lt;a href="https://buildpacks.io/"&gt;Buildpacks&lt;/a&gt;, &lt;a href="https://github.com/GoogleContainerTools/jib"&gt;Jib&lt;/a&gt;, and many more. These vary in complexity, but each one aims to streamline the container build process, particularly for developers.&lt;/p&gt; &lt;p&gt;Jib, for example, is a build tool specifically created for building container images with Java. S2I is a tool that take can take source code, do an introspection to see what kind of application it is (e.g., the presence of a &lt;code&gt;pom.xml&lt;/code&gt; would indicate that it is a Java application), and then build a container image from the application's source code using build images that are predefined by the S2I tool, or custom build images provided by the user. Buildpacks work similarly but require a buildpack builder image.&lt;/p&gt; &lt;p&gt;The downside is that you will need to install a specific command line interface (CLI) program for each of these container build tools and understand how to use them. Moreover, you will need to know what base images these build tools use (if applicable) or supply your own and figure out whether they are safe to use.&lt;/p&gt; &lt;p&gt;For further information on container terminology, Scott McCarty wrote a comprehensive introduction in &lt;a href="https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction#container_orchestration"&gt;this article&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;How Quarkus helps with Containerfiles&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus &lt;/a&gt;is a Java stack that aims to make cloud-native development easier for the Java developer. Quarkus will supply Containerfiles from the moment the developer bootstraps a Quarkus application, whether it is through the &lt;a href="https://quarkus.io/guides/cli-tooling"&gt;Quarkus CLI&lt;/a&gt;'s command &lt;code&gt;quarkus create app&lt;/code&gt;, by generating a starter project on &lt;a href="https://code.quarkus.io/"&gt;code.quarkus.io&lt;/a&gt;, or by using one of the &lt;a href="https://quarkus.io/blog/vscode-quarkus-1.13.0-released/"&gt;Quarkus plug-ins&lt;/a&gt; in VS Code or IntelliJ.&lt;/p&gt; &lt;p&gt;By bootstrapping a new Quarkus application, a folder is created in &lt;code&gt;src/main/docker&lt;/code&gt; with prebaked Containerfiles (named &lt;code&gt;Dockerfile.*&lt;/code&gt; in this case) based on the UBI images previously mentioned in this article (Figure 1).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-04-28_18-36-35.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-04-28_18-36-35.png?itok=roN3aXaF" width="317" height="224" alt="Dockerfiles supplied by Quarkus upon bootstrapping an application" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Dockerfiles supplied by Quarkus upon bootstrapping an application.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Quarkus supplies 4 different Containerfiles for different use cases:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;code&gt;Dockerfile.jvm&lt;/code&gt; is used by default to build a Java container that runs the Quarkus application in JVM mode.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;Dockerfile.legacy-jar&lt;/code&gt; is used if for the legacy-jar packaging type (&lt;a href="https://quarkus.io/blog/quarkus-1-12-0-final-released/#fast-jar-as-default"&gt;more information on legacy vs fast-jar in Quarkus&lt;/a&gt;).&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;Dockerfile.native&lt;/code&gt; uses a UBI without JVM for running natively compiled Quarkus apps.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;Dockerfile.native-micro&lt;/code&gt; is a &lt;a href="https://quarkus.io/guides/quarkus-runtime-base-image"&gt;custom built micro UBI image&lt;/a&gt; to run native Quarkus apps.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;These Containerfiles can be used with regular Docker or Podman commands, but Quarkus also enables building container images directly from build tools such as Maven or Gradle. Basically, you can build an image with, for example, &lt;code&gt;mvn package quarkus:image-build&lt;/code&gt; and Quarkus will automatically build a container image using, by default, the &lt;code&gt;Dockerfile.jvm&lt;/code&gt; that came with the Quarkus project.&lt;/p&gt; &lt;p&gt;Similarly, to build an image without a JVM that runs a natively compiled binary, one must simply add the &lt;code&gt;--Dnative&lt;/code&gt; parameter to the command (in this case, Maven) , e.g., &lt;code&gt;mvn package quarkus:image-build -Dnative&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;There's one caveat with the latter command. You will need to install the &lt;a href="https://www.graalvm.org/latest/reference-manual/native-image/"&gt;GraalVM Native Image Builder&lt;/a&gt; executable (called native-image). Quarkus, however, has an alternative way to leverage GraalVM's Native Image Build without installing the native-image executable locally. Quarkus will, in fact, detect the absence of the native-image executable and attempt to download a container image that contains the bits to do a native Java compilation and actually build the native binary in a container (provided you have Docker or Podman installed, of course). The native binary builder image is based on &lt;a href="https://developers.redhat.com/blog/2021/04/14/mandrel-a-specialized-distribution-of-graalvm-for-quarkus"&gt;Mandrel&lt;/a&gt;, a downstream distribution of the GraalVM community edition whose main goal is to provide a native-image release specifically to support Quarkus.&lt;/p&gt; &lt;h2&gt;More CLI magic with Quarkus&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://quarkus.io/guides/cli-tooling"&gt;Quarkus CLI&lt;/a&gt; is, in essence, a convenience wrapper around the Maven or Gradle build tools that let you create projects, start Quarkus in dev and test mode, manage extensions and do essential build, dev, and even deploy. It provides a way to control the full inner-loop development experience without having to switch tools.&lt;/p&gt; &lt;p&gt;Thanks to the Quarkus CLI, you can execute simple commands that are easy to remember, such as &lt;code&gt;quarkus image build&lt;/code&gt;. Behind the scenes, Quarkus will invoke the chosen build tool, package the application, and build a container image using the previously mentioned Containerfiles and a local Docker or Podman instance (as shown in Figure 2).&lt;/p&gt; &lt;p&gt;Similarly, to use either Jib or Buildpacks, you can run the same command and append the container build tool of your preference, e.g., &lt;code&gt;quarkus image build jib&lt;/code&gt;, or &lt;code&gt;quarkus image build buildpack&lt;/code&gt;.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-05-15_13-54-00.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-05-15_13-54-00.png?itok=2Ml9XAXI" width="600" height="151" alt="Quarkus container image build success message" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Image build success message after running quarkus image build.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The Quarkus CLI lets you push a container image to a registry as well with &lt;code&gt;quarkus image push&lt;/code&gt;. In fact, the application build, container build, and container push can be done in one go by adding the --also-build flag, e.g., &lt;code&gt;quarkus image push --also-build&lt;/code&gt;. Or, as a natively compiled binary, &lt;code&gt;quarkus image push --also-build --native&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Managing and running your container with Podman Desktop&lt;/h2&gt; &lt;p&gt;Once the container image has been built, you will likely want to run the container to make sure it actually works, or perhaps to use it with other applications. You could learn how to use Podman or Docker commands such as &lt;code&gt;podman run&lt;/code&gt; or &lt;code&gt;docker run;&lt;/code&gt; however, you will need to have at least some expertise in these tools to expose ports, mount volumes, execute commands, etc.&lt;/p&gt; &lt;p&gt;&lt;a href="https://podman-desktop.io"&gt;Podman Desktop&lt;/a&gt; is a UI tool that makes running and managing containers much easier with an intuitive user experience. You can install it on Windows (with WSL), macOS, and Linux, and it will install the underlying Podman utility as well, if needed.&lt;/p&gt; &lt;p&gt;You can spin up a container from the images view by clicking the play button next to the container image, as shown in Figure 2. You can then customize the container runtime configuration or let Podman automatically map the container's external port to the app's internal port based on the &lt;code&gt;EXPOSE&lt;/code&gt; value of the image (by default, &lt;code&gt;8080&lt;/code&gt; in Quarkus). Podman can automatically figure out if another process on the machine is already using that port and reassign the container's external port to use a different one instead. This is an impressive feature, in contrast to having to manually figure out if a port is being used on your machine with a command such as &lt;code&gt;netstat&lt;/code&gt; or &lt;code&gt;lsof&lt;/code&gt;, which is typically more in the realm of a system administrator.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/podman-desktop-start-container_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/podman-desktop-start-container_0.png?itok=RTAPDlKV" width="600" height="384" alt="Podman Desktop Images view with the cursor hovering over the start container button" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: Podman Desktop's Images view with "play" button to start a container.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;More Podman Desktop features&lt;/h2&gt; &lt;p&gt;Podman Desktop can do more than just run containers. For example, you might want to ssh into a container to do debugging or access or even modify settings. Using the CLI, you would have to find out the name of your container (e.g., with &lt;code&gt;podman ps&lt;/code&gt;) and then remember a command like &lt;code&gt;podman exec -it CONTAINER_ID /bin/bash&lt;/code&gt;. With Podman Desktop, it's as simple as selecting the container in the container view and clicking the &lt;strong&gt;Terminal&lt;/strong&gt; tab.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/podman-desktop-terminal.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/podman-desktop-terminal.png?itok=zQgSlTio" width="600" height="412" alt="Podman Desktop showing the container terminal view" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: Executing commands in a terminal inside a container with Podman Desktop.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Similarly, if you want to access the logs of a container, you can click the &lt;strong&gt;Logs&lt;/strong&gt; tab, or if you want to inspect the state of a container—e.g., to see what file systems it's using, what environment variables are set, etc.—then you could just click &lt;strong&gt;Inspect&lt;/strong&gt;.&lt;/p&gt; &lt;h3&gt;Using pods outside of Kubernetes&lt;/h3&gt; &lt;p&gt;Podman Desktop, as its name implies, can also run containers as pods. You could, for example, run another container, such as a caching database (Redis or Infinispan), and combine it with the application's container into a pod resulting in a single deployable and scalable unit that shares networking, just like one would run a pod on Kubernetes.&lt;/p&gt; &lt;h3&gt; Kubernetes and Podman Desktop&lt;/h3&gt; &lt;p&gt;It is possible to deploy containers or pods from Podman Desktop directly to a Kubernetes instance as well (Figure 3). To do so, simply click the rocket icon in the top right of the Podman Desktop view. Podman Desktop will detect a Kubernetes context (provided you are logged in to a Kubernetes cluster using, e.g., &lt;code&gt;kubectl login&lt;/code&gt;) and proceed to deploy the container(s) on a cluster as pods.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/podman-desktop-deploy-kubernetes.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/podman-desktop-deploy-kubernetes.png?itok=02W-YZn1" width="600" height="258" alt="Podman Desktop showing the deploy to kubernetes button" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: Podman Desktop's Logs view, with the Deploy to Kubernetes button highlighted.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;If a Kubernetes cluster is unavailable, Podman Desktop can start a Kubernetes-in-Docker (Kind) cluster through the Kind extension, available in the Podman Desktop settings.&lt;/p&gt; &lt;p&gt;Another alternative is to create a no-cost &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; instance. You can provision one by visiting &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;developers.redhat.com/developer-sandbox&lt;/a&gt; and following the Create a free OpenShift Sandbox instance. This gives you a fully functional Kubernetes instance you can use to deploy your containerized application(s) to, and perhaps even expose the application to the world with!&lt;/p&gt; &lt;h2&gt;Get started today&lt;/h2&gt; &lt;p&gt;Building and running Java containers has become much easier over the years. Using frameworks such as Quarkus and tools like Podman Desktop and OpenShift make it simple to start working with containers, even for Java developers who have are new to the cloud-native world.&lt;/p&gt; &lt;p&gt;Try it today by &lt;a href="https://quarkus.io/get-started/"&gt;creating a Quarkus application&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/container-image"&gt;building it into a container image&lt;/a&gt;, and &lt;a href="https://podman-desktop.io/docs/getting-started/starting-a-container"&gt;running it on Podman Desktop&lt;/a&gt;. &lt;/p&gt; &lt;h3&gt;Additional resources&lt;/h3&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2023/03/01/podman-desktop-introduction"&gt;What is Podman Desktop? A developer's introduction&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2023/05/23/podman-desktop-now-generally-available"&gt;Podman Desktop 1.0: Local container development made easy&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2023/06/09/deploy-and-test-kubernetes-containers-using-podman-desktop"&gt;Tutorial: Deploy and test Kubernetes containers using Podman Desktop&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/12/12/kubernetes-native-inner-loop-development-quarkus"&gt;Kubernetes-native inner loop development with Quarkus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift"&gt;Learn Quarkus faster in the Developer Sandbox for Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/28/build-your-first-java-serverless-function-using-quarkus-quick-start"&gt;Build your first Java serverless function using a Quarkus quick start&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/11/quarkus-spring-developers-kubernetes-native-design-patterns"&gt;Quarkus for Spring developers: Kubernetes-native design patterns&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/03/managing-java-containers-quarkus-and-podman-desktop" title="Managing Java containers with Quarkus and Podman Desktop"&gt;Managing Java containers with Quarkus and Podman Desktop&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Kevin Dubois</dc:creator><dc:date>2023-07-03T07:00:00Z</dc:date></entry></feed>
